\chapter{Proofs and Experimental Results}\label{ResultsChapter}
The results of my research consist of proofs and experimental performance for the implementations described in Chapter \ref{MethodsChapter}. This chapter consists of two sections, one for generalization proofs and the other describing extraction and performance. The Kernel Perceptron generalization proofs can be found in subsection \ref{KPProofs}. Subsection \ref{KPBProofs} provides the Budget Kernel Perceptron generalization, and the Description Kernel Perceptron generalization is located in subsection \ref{KPDProofs}. In Section \ref{HaskellExtractionPerformance}, the extraction directives for the three implementations are detailed in subsection \ref{DetailsHaskellExtraction} which transform the Coq implementations to Haskell modules. Subsections \ref{SyntheticResults}, \ref{IrisResults}, \ref{SonarResults} describe the testing methodology for three datasets that were used to evaluate the experimental generalization error and analyze the runtime of training and testing each implementation. Finally, in subsection \ref{ResultsDiscussion}, the trends and results seen across datasets and implementations are outlined. The conclusions and future work for this research follow in Chapter \ref{ConclusionsChapter}.
\section{Generalization Proofs}\label{Proofs}
In the MLCert framework, much of the proof burden has been automated. For a new Learner representing a machine learning algorithm, there are two new lemmas that need to be proved. The first lemma proves the cardinality of the parameters used by the algorithm, which corresponds to the size of the parameter space. The second lemma applies the first in order to prove a generalization bound for the Learner as a whole. An example of the second lemma for a generic Learner is shown in Figure \ref{LearnerLemma}.

\begin{figure}
    \caption{Generalization Bound for a generic Learner}
    \label{LearnerLemma}
    \begin{lstlisting}
Lemma Learner_bound eps (eps_gt0 : 0 < eps) init : 
    @main A B Params Hypers Learner
      hypers m m_gt0 epochs d eps init (fun _ => 1) <=
    #|Params| * exp (-2%R * eps^2 * mR m).
    \end{lstlisting}
\end{figure}

The definition of main which is used in Figure \ref{LearnerLemma} can be found in the file ``learners.v''. Once main has been instantiated with the specifics of the Learner, such as its particular Params and Hypers, there are proofs in ``learners.v'' such as the lemma main\_bound which provide the machinery necessary to prove this inequality over the real numbers. As described by Bagnall and Stewart \cite{BS19}, MLCert uses Hoeffding's inequality, a type of Chernoff bound, to prove the generalization bound for a Learner. In the following subsections, the lemmas proving the generalization bounds for the Kernel Perceptron, Budget Kernel Perceptron, and Description Kernel Perceptron will be discussed.
\subsection{Kernel Perceptron Generalization Proofs}\label{KPProofs}
The bound for the Kernel Perceptron relies on the size of the parameter space. As proven in the lemma K\_card\_Params in the section KernelPerceptronGeneralization, the cardinality of the parameters for the Kernel Perceptron is shown in Equation \ref{KPParams}. This power of 2 is calculated by unfolding the definition of Params, which consists of the training set and a float array of size m. As all values are floating point numbers stored in 32 bits, the cardinality of a single floating point number is $2^{32}$. Therefore, as the dimensions of the training set are equal to m training examples multiplied by n dimensions, the cardinality of the training set is equal to $2^{m*n*32}$. The cardinality of a float array of size m is $2^{m * 32}$. 

\begin{equation} \label{KPParams}
 \#|Params| = 2^{(m*n*32 + m*32)}
\end{equation}

Kcard\_Params is central to the proof of the generalization bounds of the Kernel Perceptron in the lemma KPerceptron\_bound, which is defined in Figure \ref{KPLemma}. The generalization bound for the Kernel Perceptron is very loose, as growth in the size of the training set causes exponential growth in the generalization error. This limits the usefulness of the Kernel Perceptron's generalization bound, as a loose generalization bound provides few guarantees for performance or correctness. However, the Kernel Perceptron bound allows for comparison between this result and the generalization bounds for the Budget Kernel Perceptron and the Description Kernel Perceptron. 

\begin{figure}
    \caption{Generalization Bound for the Kernel Perceptron}
    \label{KPLemma}
    \begin{lstlisting}
Lemma Kperceptron_bound eps (eps_gt0 : 0 < eps) init : 
    @main A B Params KernelPerceptron.Hypers 
      (@KernelPerceptron.Learner n m KPsupport_vectors H K)
      hypers m m_gt0 epochs d eps init (fun _ => 1) <=
    2^(m*n*32 + m*32) * exp (-2%R * eps^2 * mR m).
    \end{lstlisting}
\end{figure}

\subsection{Budget Kernel Perceptron Generalization Proofs}\label{KPBProofs}
The Budget Kernel Perceptron has a similar bound on the cardinality of the parameter space. However, the parameter space for the Budget Kernel Perceptron is not dependent on m, the size of the training set, whatsoever. Instead, the parameter space relies on (S sv), which is the size of the support set. The successor of sv is used to denote the size of the support set so that the budget update procedure is always possible regardless of the value of sv, as there will be at least one support vector able to be replaced.
\\Like the Kernel Perceptron, the Budget Kernel Perceptron stores the support set and a float array. The float array is of size (S sv), so its cardinality is $2^{32 * (S sv)}$. The support set stores (S sv) training examples, which consist of one float value for each of the n dimensions of the data, plus a Boolean value for the label of the support vector. Therefore, the cardinality of each training example in the support set is $2^{1 + n * 32}$. The full cardinality of the Budget Kernel Perceptron Params is given in Equation \ref{KPBParams}. The lemma proving this bound is found in the section KernelPerceptronGeneralizationBudget, named Kcard\_Params\_Budget.

\begin{equation} \label{KPBParams}
 \#|Params| = 2^{((32*(S sv) + ((1 + n * 32)*(S sv))))}
\end{equation}

Figure \ref{KPBLemma} shows the lemma for the generalization bound of the Budget Kernel Perceptron, which uses Kcard\_Params\_Budget in its proof. Comparing the bound of the Budget Kernel Perceptron to the Kernel Perceptron, the overall structure of the two bounds is similar when the number of training examples is the same. However, because the support set can be significantly smaller than the number of training examples, the Budget Kernel Perceptron's bound is tighter than that of the base Kernel Perceptron. 

\begin{figure}
    \caption{Generalization Bound for the Budget Kernel Perceptron}
    \label{KPBLemma}
    \begin{lstlisting}
Lemma Kperceptron_bound_budget eps (eps_gt0 : 0 < eps) init : 
    @main A B Params KernelPerceptronBudget.Hypers 
      (@KernelPerceptronBudget.Learner n sv F K U)
      hypers m m_gt0 epochs d eps init (fun _ => 1) <=
    INR 2^((32*(S sv) + ((1 + n * 32)*(S sv)))) * exp (-2%R * eps^2 * mR m).
    \end{lstlisting}
\end{figure}

\subsection{Description Kernel Perceptron Generalization Proofs}\label{KPDProofs}

The generalization bound for the Description Kernel Perceptron is similar to that of the Budget Kernel Perceptron. First, we must define the cardinality of the parameter space used by the Description Kernel Perceptron. The parameters store a single float32 value paired with (S des) support vectors. The successor of des is used as the size of the support set, so that support vector replacement is always possible. Like with the Budget Kernel Perceptron, the cardinality of each support vector in the support set is $2^{1 + n * 32}$, which stores a single Boolean label as well as $n$ 32-bit floating point values. However, because there is not a float value for every support vector, only the cardinality of a single float32 must be added to the cardinality of the entire support set. The cardinality of the Description parameters is shown in Equation \ref{KPDParams}.

\begin{equation} \label{KPDParams}
 \#|Params| = 2^{((32 + ((1 + n * 32)*(S des))))}
\end{equation}

Figure \ref{KPDLemma} shows the generalization bound of the Description Kernel Perceptron in the lemma Kperceptron\_bound\_Des. This bound is similar to the bound of the Budget Kernel Perceptron, but is tighter because there is only one float32 value instead of a float32 value per support vector. This small difference means that if the budget is the same as the number of mistakes, the Description Kernel Perceptron will have a slightly tighter bound. 

\begin{figure}
    \caption{Generalization Bound for the Description Kernel Perceptron}
    \label{KPDLemma}
    \begin{lstlisting}
Lemma Kperceptron_bound_Des eps (eps_gt0 : 0 < eps) init : 
    @main A B Params KernelPerceptronDes.Hypers 
      (@KernelPerceptronDes.Learner n des F K)
      hypers m m_gt0 epochs d eps init (fun _ => 1) <=
    INR 2 ^ (32 + (1 + n * 32) * (S des)) * exp (-2%R * eps^2 * mR m).
    \end{lstlisting}
\end{figure}

\section{Haskell Extraction and Performance Experiments}\label{HaskellExtractionPerformance}
In order for the Coq implementations to be run, these implementations must be extracted to Haskell. The file ``extraction\_hs.v'' contains extraction directives for Haskell so that some Coq functions and data structures are extracted properly. The last Coq module for each implementation uses the extractible\_main definition, found in ``learners.v'', to also provide the necessary machinery that Learner.t relies on. The extracted Coq code is written to a Haskell file located in the directory hs in MLCert.
\\The extracted Haskell code for a machine learning algorithm does not contain code to initialize the system with training and testing data or functions to display accuracy and generalization error results to the user. Unverified Haskell drivers have been written for these implementations, which include the extracted Haskell code as a module. The Haskell drivers for the Kernel Perceptron implementations can also be found in the hs/ directory.
\subsection{Details of Haskell Extraction}\label{DetailsHaskellExtraction}
The extraction directives for the Kernel Perceptron can be found in the section KPerceptronExtraction in the file ``kernelperceptron.v''. This section extracts the Kernel Perceptron to the Haskell file ``KPerceptron.hs'' in the hs directory. There are two Haskell driver files for the Kernel Perceptron. The first driver is a small driver to test that the Kernel Perceptron using a quadratic kernel can classify the XOR function with 100\% accuracy. The linear kernel cannot be used because the XOR function is not linearly separable. The four samples for this function are specified in the driver, along with the quadratic kernel for the prediction function. When run, this driver demonstrates that the Kernel Perceptron behaves as expected with the quadratic kernel and is able to classify data that is not linearly separable.
\\The second driver, ``KPerceptronTest.hs'', is a more general test of the Kernel Perceptron. For this driver, the dimensionality of the data is three, and each of the three values is randomly generated in the range (-1.0, 1.0). The number of training examples is 200, and one sample is chosen as the hyperplane which is used to label each sample as either True or False. The Kernel Perceptron runs for five epochs.
\\The section KPerceptronExtractionBudget in ``kernelperceptron.v'' contains the extraction directives for the Budget Kernel Perceptron, which extracts the Budget Kernel Perceptron to the Haskell file ``KPerceptronBudget.hs''. The driver file ``KPerceptronBudgetTest.hs'' is similar to the second driver of the Kernel Perceptron. However, while the dimensionality, number of training examples, and epochs are kept the same, the number of support vectors is specified as 20, limiting the size of the support set to 10\% of the training set. The support set is initialized to zero vectors, which will be removed as the Budget Kernel Perceptron makes classification errors during training. 
\\This is a placeholder until the Description Kernel Perceptron is implemented.
\subsection{Synthetic Dataset Performance Results}\label{SyntheticResults}
\subsection{Iris Dataset Performance Results}\label{IrisResults}
\subsection{Sonar Mines vs. Rocks Dataset Performance Results}\label{SonarResults}
\subsection{Discussion of Generalization Error and Timing Results}\label{ResultsDiscussion}
\section{Chapter Summary}\label{ResultsChapterSummarySection}
These results from the implementations of the Kernel Perceptron, Budget Kernel Perceptron, and Description Kernel Perceptron demonstrate that the generalization error for the Kernel Perceptron can be improved through limiting the size of the support set or placing a limit on the number of mistakes made during training. The conclusions drawn from these results are described in Chapter \ref{ConclusionsChapter}, along with a discussion of future work to be done in the field of machine learning verification.
