\chapter{Introduction}

The field of machine learning research has advanced rapidly in the past decade. Machine learning describes the class of computer programs that automatically learn from experience, often employed for classification, recognition, and clustering tasks. One of the classic problems in machine learning is digit recognition to classify handwritten numbers automatically. Computers have historically struggled to interpret handwritten information because handwriting can vary drastically between writers. While humans can be taught to read as well as learn to read on their own, handwriting recognition can be challenging for computers to accomplish. Several datasets have been created to provide a common source of handwritten digit data so that the performance of different machine learning algorithms can be directly compared. For example, the MNIST dataset \cite{LBBH98} is one of the primary datasets for computers to learn how to classify handwritten digits into the numbers 0-9. This dataset allows researchers to compare the performance of multiple models, trained and tested on the same data, but using different machine learning algorithms. Some systems have achieved a near-perfect performance on the MNIST dataset for the problem of handwritten digit classification, and this technology is valuable for processing documents, such as ZIP codes on letters sent through the U.S. Postal Service \cite{MKB17}.
\\Increasingly, machine learning has been heavily integrated into our daily lives. As described by the authors of ``Social media big data analytics", social media companies such as Facebook, Twitter, and YouTube learn from our digital data in order to serve individuals with targeted information and often advertising \cite{GHHA19}. Retailers track customer purchases to learn about individuals' habits and entice them with specific offers and coupons. The pages we visit, profiles we create, and products we buy are used to predict our future actions and monetize our attention. This kind of task would be almost impossible for a human to complete, due to the vast amounts of data involved per person or account. In addition to social media, retailers, and advertisers, machine learning techniques are also being employed in critical systems, such as healthcare and infrastructure, where failure can lead to the loss of time, money, and lives. Research to evaluate the use and oversight of machine learning algorithms \cite{Var16} has shown that there are few existing safety principles and regulations for critical systems that rely on machine learning components. Machine learning drives more than websites and commerce; its algorithms are also responsible for the well-being and safety of people around the world, and regulation has largely not caught up with machine learning advances.
\\The details of machine learning differ from algorithm to algorithm, but for most methods, machine learning algorithms learn models from training data to encode the knowledge implicit in the training data. Models consist of learned parameters, which represent different kinds of data depending on the encoding of the model, and hyperparameters, a small number of variables directly specified by the programmer that may control the speed of training or other high level details. Models tend to be complex, and can require thousands or millions of learned parameters for high accuracy on a given problem. Learned models are able to take a new piece of data as input and produce a result or judgment from that data. For the recognition of handwritten digits, the input to the model is the handwritten digit, and the output is the classification of that digit as a number from 0-9.
\\The development of new machine learning algorithms or advances in training and validation techniques tends to be experimentally driven in most applications. New or finely tuned configurations for internal components can lead to increased accuracy and efficiency or decreased training time compared to other algorithms for a specific problem or dataset. Small refinements to algorithms and hyperparameters can have enormous impacts on training time, model size, and performance on unseen testing data. Because of the complexity of the models produced by many machine learning algorithms, many new papers published in the field describe results found through experimentation, as opposed to examining the underlying theory responsible for these advances. Additional research in understanding the theory behind machine learning may help to understand why some techniques are better suited for some problems than others, as well as potential avenues for exploration.
\\Finding errors in machine learning algorithms or models can be very difficult. With thousands or millions of parameters learned by the computer, not specified by the programmer, algorithms can easily get stuck in small, local solutions instead of finding the optimal solution. For example, gradient descent is the standard training method for neural networks that minimizes the error in the network's model over the training data. To visualize the process of gradient descent, the algorithm seeks to find the lowest, or global, minimum of a multi-dimensional hillside with many peaks and valleys. Through many iterations, gradient descent travels downward along the gradient until a place is reached where descent is no longer possible. If the algorithm cannot find a deeper valley, this depth is returned as the overall solution. However, gradient descent can fail to find the global solution when the hyperparameters are not tuned correctly by the programmer or deeper valleys take too long to find, which can occur for nonconvex optimization problems. Techniques have been developed to mitigate the limitations of gradient descent, such as momentum, but the programmer usually has to experiment with multiple techniques to achieve peak performance. Additionally, few machine learning algorithms have theoretical properties that can be verified, such as a theorem that a learning algorithm will always terminate or find the global solution. Research into verifying machine learning to produce models with optimal behavior is limited due to these difficulties.
\\One way to increase our knowledge in the theory of machine learning is to verify the correctness of machine learning algorithms through mathematical, machine-checked proofs. Formal verification often employs proof assistants, such as Coq, which allow for the integration of proofs with software specifications and implementations. Mathematical proofs in Coq are guaranteed to be as valid as the proof assistant itself and the correctness of the specifications and theorems proved. Proofs are implemented as portable programs, which allows for others to verify proofs independently. Because proofs and implementations are written in the same environment, the proofs directly correspond to the implementation verified. The Coq environment also provides libraries containing both implementations of data structures and proofs to aid in the development of verified systems.
\\Researchers have used the Coq proof assistant to verify many different software systems and prove correctness properties. The CompCert compiler for the C language \cite{Ler09} is the first realistic verified compiler, proving that the behavior of a C program compiled with CompCert will not be changed in the transformation of compilation. Verified compilers ensure that the executable program produced by the compiler does not contain errors produced in compilation. For safety-critical applications, one might argue that executables created by a verified compiler are more secure than executables created by unverified compilers. Another verified system written in Coq is Verdi \cite{WWP15}, a framework for specifying and implementing distributed systems with tolerance for node faults. In a network of computers, connections can be dropped, packets lost or sent out of order, and nodes can fail or restart. Verdi allows the programmer to specify the fault conditions their distributed system should be resilient against, and the Verdi system itself mechanizes much of the proof process and code extraction for deployment in real-world networks. Distributed system software written with Verdi has been verified to handle faults and errors that may occur. Finally, the CertiKOS project \cite{GSC16} has developed several microkernels with security properties and proofs of correctness, including mC2, a verified concurrent microkernel. Operating systems allocate memory and computer resources and must defend against malicious processes. Coq has been used to specify and verify a diverse range of algorithms, data structures, and applications beyond these three projects.
\\In this thesis, I will describe my additions to the verification framework MLCert \cite{MLC}. MLCert provides software tools and libraries in the Coq proof assistant for verified machine learning in Coq, such as generic definitions and proofs for machine learning algorithms, example algorithms such as the Perceptron, and extensions for the implementation and training of neural networks. Building on the Perceptron implementation and existing proofs in MLCert, I present a verified implementation of the Kernel Perceptron algorithm, as well as two variants on the Kernel Perceptron algorithm: a Budget Kernel Perceptron and a Description Kernel Perceptron. Background information for this thesis is provided in Chapter 2, with an introduction to the Perceptron and Kernel Perceptron algorithms, a more extended discussion of the challenges and tactics of machine learning verification, and motivation for the Budget Kernel Perceptron and Description Kernel Perceptron algorithms. Chapter 3 describes the methodology for implementing these algorithms in Coq. The proofs for these implementations and their performance results are detailed in Chapter 4. Finally, future work and conclusions are discussed in Chapter 5.

